{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on the CPU\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_epoch = 100\n",
    "data_dir = \"C:/Users/hjane/Desktop/Git/Unet/data\"\n",
    "ckpt_dir = \"C:/Users/hjane/Desktop/Git/Unet/checkpoint\"\n",
    "log_dir = \"C:/Users/hjane/Desktop/Git/Unet/log\"\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"running on the CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1200/1*qNdglJ1ORP3Gq77MmBLhHQ.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        def CBR2d(in_channnels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channnels, out_channels=out_channels, \n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)]\n",
    "            \n",
    "            layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            layers += [nn.ReLU()]\n",
    "            \n",
    "            cbr = nn.Sequential(*layers)\n",
    "            \n",
    "            return cbr\n",
    "\n",
    "        # Contracting path\n",
    "        self.enc1_1 = CBR2d(in_channnels=1, out_channels=64)\n",
    "        self.enc1_2 = CBR2d(in_channnels=64, out_channels=64)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.enc2_1 = CBR2d(in_channnels=64, out_channels=128)\n",
    "        self.enc2_2 = CBR2d(in_channnels=128, out_channels=128)\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.enc3_1 = CBR2d(in_channnels=128, out_channels=256)\n",
    "        self.enc3_2 = CBR2d(in_channnels=256, out_channels=256)\n",
    "        \n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.enc4_1 = CBR2d(in_channnels=256, out_channels=512)\n",
    "        self.enc4_2 = CBR2d(in_channnels=512, out_channels=512)\n",
    "        \n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.enc5_1 = CBR2d(in_channnels=512, out_channels=1024)\n",
    "        \n",
    "        # Expansive path\n",
    "        self.dec5_1 = CBR2d(in_channnels=1024, out_channels=512)\n",
    "        \n",
    "        self.uppool4 = nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=2, stride=2, padding=0, bias=True)\n",
    "        \n",
    "        self.dec4_2 = CBR2d(in_channnels=2 * 512, out_channels=512)\n",
    "        self.dec4_1 = CBR2d(in_channnels=512, out_channels=256)\n",
    "        \n",
    "        self.uppool3 = nn.ConvTranspose2d(in_channels=256, out_channels=256, kernel_size=2, stride=2, padding=0, bias=True)\n",
    "        \n",
    "        self.dec3_2 = CBR2d(in_channnels=2*256, out_channels=256)\n",
    "        self.dec3_1 = CBR2d(in_channnels=256, out_channels=128)\n",
    "        \n",
    "        self.uppool2 = nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=2, stride=2, padding=0, bias=True)\n",
    "        \n",
    "        self.dec2_2 = CBR2d(in_channnels=2*128, out_channels=128)\n",
    "        self.dec2_1 = CBR2d(in_channnels=128, out_channels=64)\n",
    "        \n",
    "        self.uppool1 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=2, stride=2, padding=0, bias=True)\n",
    "        \n",
    "        self.dec1_2 = CBR2d(in_channnels=2*64, out_channels=64)\n",
    "        self.dec1_1 = CBR2d(in_channnels=64, out_channels=64)\n",
    "        \n",
    "        self.fc = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "        \n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "        \n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "        \n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "        \n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        \n",
    "        dec5_1 =self.dec5_1(enc5_1)\n",
    "        \n",
    "        uppool4 = self.uppool4(dec5_1)\n",
    "        \n",
    "        dec4_2 = self.dec4_2(torch.cat((uppool4, enc4_2), dim=1))\n",
    "        dec4_1 = self.dec4_1(dec4_2)\n",
    "        \n",
    "        uppool3 = self.uppool3(dec4_1)\n",
    "        \n",
    "        dec3_2 = self.dec3_2(torch.cat((uppool3, enc3_2), dim=1))\n",
    "        dec3_1 = self.dec3_1(dec3_2)\n",
    "        \n",
    "        uppool2 = self.uppool2(dec3_1)\n",
    "        \n",
    "        dec2_2 = self.dec2_2(torch.cat((uppool2, enc2_2), dim=1))\n",
    "        dec2_1 =self.dec2_1(dec2_2)\n",
    "        \n",
    "        uppool1 = self.uppool1(dec2_1)\n",
    "        \n",
    "        dec1_2 = self.dec1_2(torch.cat((uppool1, enc1_2), dim=1))\n",
    "        dec1_1 = self.dec1_1(dec1_2)\n",
    "        \n",
    "        x = self.fc(dec1_1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernel_size(Convolution의 view, 2차원에서는 대부분 3x3 pixel), stride(이미지를 스캔할 때 kernel의 step size), bias는 CBR layer를 구축하는데 있어 값이 고정됨 >> 이 값들이 뭐하는 얘들이지?\n",
    "각각의 함수가 의미하는 transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        lst_data = os.listdir(self.data_dir)\n",
    "\n",
    "        lst_label = [f for f in lst_data if f.startswith('label')]\n",
    "        lst_input = [f for f in lst_data if f.startswith('input')]\n",
    "\n",
    "        lst_label.sort()\n",
    "        lst_input.sort()\n",
    "\n",
    "        self.lst_label = lst_label\n",
    "        self.lst_input = lst_input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lst_label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = np.load(os.path.join(self.data_dir, self.lst_label[index]))\n",
    "        input = np.load(os.path.join(self.data_dir, self.lst_input[index]))\n",
    "\n",
    "        label = label/255.0\n",
    "        input = input/255.0\n",
    "\n",
    "        if label.ndim == 2:\n",
    "            label = label[:, :, np.newaxis]\n",
    "        if input.ndim == 2:\n",
    "            input = input[:, :, np.newaxis]\n",
    "\n",
    "        data = {'input': input, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "class ToTensor(object):\n",
    "    def __call__(self, data):\n",
    "        label, input = data['label'], data['input']\n",
    "\n",
    "        label = label.transpose((2, 0, 1)).astype(np.float32)\n",
    "        input = input.transpose((2, 0, 1)).astype(np.float32)\n",
    "\n",
    "        data = {'label': torch.from_numpy(label), 'input': torch.from_numpy(input)}\n",
    "\n",
    "        return data\n",
    "        \n",
    "class Normalization(object):\n",
    "    def __init__(self, mean=0.5, std=0.5):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        label, input = data['label'], data['input']\n",
    "        \n",
    "        input = (input - self.mean) / self.std\n",
    "        \n",
    "        data = {'label' : label, 'input' : input}\n",
    "        \n",
    "        return data\n",
    "    \n",
    "class RandomFlip(object):\n",
    "    def __call__(self, data, probability=0.5):\n",
    "        label, input = data['label'], data['input']\n",
    "        \n",
    "        if np.random.rand() < probability:\n",
    "            label = np.fliplr(label)\n",
    "            input = np.fliplr(input)\n",
    "            \n",
    "        if np.random.rand() < probability:\n",
    "            label = np.flipud(label)\n",
    "            input = np.flipud(label)\n",
    "            \n",
    "        data = {'label' : label, 'input': input}\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    Normalization(mean=0.5, std=0.5),\n",
    "    RandomFlip(),\n",
    "    ToTensor(),\n",
    "])\n",
    "dataset_train = Dataset(data_dir=os.path.join(data_dir, 'train'), transform=transform)\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "dataset_val = Dataset(data_dir=os.path.join(data_dir, 'val'), transform=transform)\n",
    "loader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build network\n",
    "net = UNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "fn_loss = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimizer\n",
    "optim = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables\n",
    "num_data_train = len(dataset_train)\n",
    "num_data_val = len(dataset_val)\n",
    "\n",
    "num_batch_train = np.ceil(num_data_val / batch_size)\n",
    "num_batch_val = np.ceil(num_data_val / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub functions\n",
    "\n",
    "fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\n",
    "fn_denorm = lambda x, mean, std: (x * std) + mean\n",
    "fn_class = lambda x: 1.0 * (x > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set SummaryWriter for Tensorboard\n",
    "writer_train = SummaryWriter(log_dir=os.path.join(log_dir, 'train'))\n",
    "writer_val = SummaryWriter(log_dir=os.path.join(log_dir, 'val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save network\n",
    "def save(ckpt_dir, net, optim, epoch):\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "        \n",
    "    torch.save({'net' : net.state_dict(), 'optim': optim.state_dict()}, './%s/model_epoch%d.pth' % (ckpt_dir, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load network\n",
    "def load(ckpt_dir, net, optim):\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        epoch = 0\n",
    "        return net, optim, epoch\n",
    "    ckpt_lst = os.listdir(ckpt_dir)\n",
    "    ckpt_lst.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "    \n",
    "    dict_model = torch.load('./%s/%s' %(ckpt_dir, ckpt_lst[-1]))\n",
    "    \n",
    "    net.load_state_dict(dict_model['net'])\n",
    "    optim.load_state_dict(dict_model['optim'])\n",
    "    epoch = int(ckpt_lst[-1].split('epoch')[1].split('pth')[0])\n",
    "\n",
    "    return net, optim, epoch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_epoch =0\n",
    "net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch 0001 / 0100 | Batch 0001 / 0001 | Loss 0.6765\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hjane\\Desktop\\Git\\Unet\\train.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hjane/Desktop/Git/Unet/train.ipynb#ch0000020?line=10'>11</a>\u001b[0m label \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hjane/Desktop/Git/Unet/train.ipynb#ch0000020?line=11'>12</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hjane/Desktop/Git/Unet/train.ipynb#ch0000020?line=13'>14</a>\u001b[0m output \u001b[39m=\u001b[39m net(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hjane/Desktop/Git/Unet/train.ipynb#ch0000020?line=15'>16</a>\u001b[0m \u001b[39m# backward pass\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hjane/Desktop/Git/Unet/train.ipynb#ch0000020?line=17'>18</a>\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\hjane\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\hjane\\Desktop\\Git\\Unet\\train.ipynb Cell 4'\u001b[0m in \u001b[0;36mUNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hjane/Desktop/Git/Unet/train.ipynb#ch0000003?line=71'>72</a>\u001b[0m pool2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool2(enc2_2)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hjane/Desktop/Git/Unet/train.ipynb#ch0000003?line=73'>74</a>\u001b[0m enc3_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc3_1(pool2)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hjane/Desktop/Git/Unet/train.ipynb#ch0000003?line=74'>75</a>\u001b[0m enc3_2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menc3_2(enc3_1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hjane/Desktop/Git/Unet/train.ipynb#ch0000003?line=75'>76</a>\u001b[0m pool3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool3(enc3_2)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hjane/Desktop/Git/Unet/train.ipynb#ch0000003?line=77'>78</a>\u001b[0m enc4_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc4_1(pool3)\n",
      "File \u001b[1;32mc:\\Users\\hjane\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hjane\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\hjane\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hjane\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/batchnorm.py?line=160'>161</a>\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/batchnorm.py?line=162'>163</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/batchnorm.py?line=163'>164</a>\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/batchnorm.py?line=164'>165</a>\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/batchnorm.py?line=165'>166</a>\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/batchnorm.py?line=166'>167</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/batchnorm.py?line=167'>168</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/batchnorm.py?line=168'>169</a>\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/batchnorm.py?line=169'>170</a>\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/batchnorm.py?line=170'>171</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/batchnorm.py?line=171'>172</a>\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/batchnorm.py?line=172'>173</a>\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/batchnorm.py?line=173'>174</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/batchnorm.py?line=174'>175</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/batchnorm.py?line=175'>176</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/batchnorm.py?line=176'>177</a>\u001b[0m     bn_training,\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/batchnorm.py?line=177'>178</a>\u001b[0m     exponential_average_factor,\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/batchnorm.py?line=178'>179</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/batchnorm.py?line=179'>180</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hjane\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2421\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/functional.py?line=2417'>2418</a>\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/functional.py?line=2418'>2419</a>\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m-> <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/functional.py?line=2420'>2421</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/functional.py?line=2421'>2422</a>\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[0;32m   <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/functional.py?line=2422'>2423</a>\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train nework\n",
    "\n",
    "st_epoch =0\n",
    "net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n",
    "\n",
    "for epoch in range(st_epoch + 1, num_epoch +1):\n",
    "    net.train()\n",
    "    loss_arr = []\n",
    "    for batch, data in enumerate(loader_train, 1):\n",
    "        # forward pass\n",
    "        label = data['label'].to(device)\n",
    "        input = data['input'].to(device)\n",
    "        \n",
    "        output = net(input)\n",
    "        \n",
    "        # backward pass\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        \n",
    "        loss = fn_loss(output, label)\n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()\n",
    "        \n",
    "        # calculate loss function\n",
    "        loss_arr +=[loss.item()]\n",
    "        \n",
    "        print(\"Train: Epoch %04d / %04d | Batch %04d / %04d | Loss %.4f\" %(epoch, num_epoch, batch, num_batch_train, np.mean(loss_arr)))\n",
    "        \n",
    "        #save Tensorboard\n",
    "        \n",
    "        label = fn_tonumpy(label)\n",
    "        input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
    "        output = fn_tonumpy(fn_class(output))\n",
    "        \n",
    "        writer_train.add_image('label', label, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
    "        writer_train.add_image('input', input, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
    "        writer_train.add_image('output', output, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
    "    \n",
    "    writer_val.add_scalar('loss', np.mean(loss_arr), epoch)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        loss_arr = []\n",
    "        \n",
    "        for batch, data in enumerate(loader_val, 1):\n",
    "            \n",
    "            # forward pass\n",
    "            label = data['label'].to(device)\n",
    "            input = data['input'].to(device)\n",
    "            \n",
    "            output = net(input)\n",
    "            \n",
    "            #calculate loss function\n",
    "            loss = fn_loss(output, label)\n",
    "            \n",
    "            loss_arr += [loss.item()]\n",
    "            \n",
    "            print(\"Valid: Epoch %04d / %04d Batch $04d / %04d | Loss %.4f\" %(epoch, num_epoch, batch, num_batch_val, np.mean(loss_arr)))\n",
    "            \n",
    "            # save Tensorboard \n",
    "            label = fn_tonumpy(label)\n",
    "            input = fn_tonumpy(fn_denorm(input))\n",
    "            output = fn_tonumpy(fn_class(output))\n",
    "            \n",
    "            writer_train.add_image('label', label, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
    "            writer_train.add_image('input', input, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
    "            writer_train.add_image('output', output, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
    "    writer_val.add_scalar('loss', np.mean(loss_arr), epoch)\n",
    "    \n",
    "    save(ckpt_dir=ckpt_dir, net=net, optim=optim, epoch=epoch)\n",
    "writer_train.close()\n",
    "writer_val.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02eddd94fd9f0d372956d3dca77a19e74c96a8fd13e5de75381a35868b9e5fc1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
