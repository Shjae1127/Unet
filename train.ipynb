{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets\n",
    "from tensorboardX import SummaryWriter\n",
    "import argparse\n",
    "from model import UNet          # UNet model\n",
    "from dataset import *               # Dataset class & transform function\n",
    "from utils import *                     # save & load function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Train the Unet\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument(\"--lr\", default=1e-3, type=float, dest=\"lr\")\n",
    "parser.add_argument(\"--batch_size\", default=4, type=int, dest=\"batch_size\")\n",
    "parser.add_argument(\"--num_epoch\", default=100, type=int, dest=\"num_epoch\")\n",
    "parser.add_argument(\"--data_dir\", default=os.path.abspath('.')+\"/data\",\n",
    "                    type=str, dest=\"data_dir\")\n",
    "parser.add_argument(\"--ckpt_dir\", default=os.path.abspath('.')+\"/checkpoint\",\n",
    "                    type=str, dest=\"ckpt_dir\")\n",
    "parser.add_argument(\"--log_dir\", default=os.path.abspath('.') + \"/log\",\n",
    "                    type=str, dest=\"log_dir\")\n",
    "parser.add_argument(\"--result_dir\", default=os.path.abspath('.')+\"/results\", type=str, dest=\"result_dir\")\n",
    "parser.add_argument(\"--mode\", default=\"train\", type=str, dest=\"mode\")\n",
    "parser.add_argument(\"--train_continue\", default=\"off\", type=str, dest=\"train_continue\")\n",
    "args = parser.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on the CPU\n"
     ]
    }
   ],
   "source": [
    "lr = args.lr\n",
    "batch_size = args.batch_size\n",
    "num_epoch = args.num_epoch\n",
    "data_dir = args.data_dir\n",
    "ckpt_dir = args.ckpt_dir\n",
    "log_dir = args.log_dir\n",
    "result_dir = args.result_dir\n",
    "mode = args.mode\n",
    "train_continue = args.train_continue\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"running on the CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1200/1*qNdglJ1ORP3Gq77MmBLhHQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernel_size(Convolution의 view, 2차원에서는 대부분 3x3 pixel), stride(이미지를 스캔할 때 kernel의 step size), bias는 CBR layer를 구축하는데 있어 값이 고정됨 >> 이 값들이 뭐하는 얘들이지?\n",
    "각각의 함수가 의미하는 transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(os.path.join(result_dir, \"png\"))\n",
    "    os.makedirs(os.path.join(result_dir, \"numpy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"train\":\n",
    "    transform = transforms.Compose([\n",
    "        Normalization(mean=0.5, std=0.5),\n",
    "        RandomFlip(),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "    dataset_train = Dataset(data_dir=os.path.join(data_dir, 'train'), transform=transform)\n",
    "    loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    dataset_val = Dataset(data_dir=os.path.join(data_dir, 'val'), transform=transform)\n",
    "    loader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    # set variables\n",
    "    num_data_train = len(dataset_train)\n",
    "    num_data_val = len(dataset_val)\n",
    "\n",
    "    num_batch_train = np.ceil(num_data_val / batch_size)\n",
    "    num_batch_val = np.ceil(num_data_val / batch_size)\n",
    "    \n",
    "else:\n",
    "    transform = transforms.Compose([\n",
    "        Normalization(mean=0.5, std=0.5),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "    dataset_test = Dataset(data_dir=os.path.join(data_dir, 'test'), transform=transform)\n",
    "    loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    # set variables\n",
    "    num_data_test = len(dataset_test)\n",
    "\n",
    "    num_batch_test = np.ceil(num_data_test / batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build network\n",
    "net = UNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "fn_loss = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimizer\n",
    "optim = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables\n",
    "num_data_train = len(dataset_train)\n",
    "num_data_val = len(dataset_val)\n",
    "\n",
    "num_batch_train = np.ceil(num_data_val / batch_size)\n",
    "num_batch_val = np.ceil(num_data_val / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub functions\n",
    "\n",
    "fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\n",
    "fn_denorm = lambda x, mean, std: (x * std) + mean\n",
    "fn_class = lambda x: 1.0 * (x > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set SummaryWriter for Tensorboard\n",
    "writer_train = SummaryWriter(log_dir=os.path.join(log_dir, 'train'))\n",
    "writer_val = SummaryWriter(log_dir=os.path.join(log_dir, 'val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_epoch =0\n",
    "net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hjane\\Desktop\\Git\\Unet\\train.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hjane/Desktop/Git/Unet/train.ipynb#ch0000019?line=17'>18</a>\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hjane/Desktop/Git/Unet/train.ipynb#ch0000019?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m fn_loss(output, label)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hjane/Desktop/Git/Unet/train.ipynb#ch0000019?line=20'>21</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hjane/Desktop/Git/Unet/train.ipynb#ch0000019?line=22'>23</a>\u001b[0m optim\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hjane/Desktop/Git/Unet/train.ipynb#ch0000019?line=24'>25</a>\u001b[0m \u001b[39m# calculate loss function\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hjane\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\hjane\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/hjane/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train nework\n",
    "\n",
    "st_epoch =0\n",
    "\n",
    "if mode == \"train\":\n",
    "    if train_continue == \"on\":\n",
    "        net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n",
    "    for epoch in range(st_epoch + 1, num_epoch +1):\n",
    "        net.train()\n",
    "        loss_arr = []\n",
    "        for batch, data in enumerate(loader_train, 1):\n",
    "            # forward pass\n",
    "            label = data['label'].to(device)\n",
    "            input = data['input'].to(device)\n",
    "            \n",
    "            output = net(input)\n",
    "            \n",
    "            # backward pass\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            \n",
    "            loss = fn_loss(output, label)\n",
    "            loss.backward()\n",
    "            \n",
    "            optim.step()\n",
    "            \n",
    "            # calculate loss function\n",
    "            loss_arr +=[loss.item()]\n",
    "            \n",
    "            print(\"Train: Epoch %04d / %04d | Batch %04d / %04d | Loss %.4f\" %(epoch, num_epoch, batch, num_batch_train, np.mean(loss_arr)))\n",
    "            \n",
    "            #save Tensorboard\n",
    "            \n",
    "            label = fn_tonumpy(label)\n",
    "            input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
    "            output = fn_tonumpy(fn_class(output))\n",
    "            \n",
    "            writer_train.add_image('label', label, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
    "            writer_train.add_image('input', input, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
    "            writer_train.add_image('output', output, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
    "        \n",
    "        writer_val.add_scalar('loss', np.mean(loss_arr), epoch)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            loss_arr = []\n",
    "            \n",
    "            for batch, data in enumerate(loader_val, 1):\n",
    "                \n",
    "                # forward pass\n",
    "                label = data['label'].to(device)\n",
    "                input = data['input'].to(device)\n",
    "                \n",
    "                output = net(input)\n",
    "                \n",
    "                #calculate loss function\n",
    "                loss = fn_loss(output, label)\n",
    "                \n",
    "                loss_arr += [loss.item()]\n",
    "                \n",
    "                print(\"Valid: Epoch %04d / %04d Batch %04d / %04d | Loss %.4f\" %(epoch, num_epoch, batch, num_batch_val, np.mean(loss_arr)))\n",
    "                \n",
    "                # save Tensorboard \n",
    "                label = fn_tonumpy(label)\n",
    "                input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
    "                output = fn_tonumpy(fn_class(output))\n",
    "                \n",
    "                writer_train.add_image('label', label, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
    "                writer_train.add_image('input', input, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
    "                writer_train.add_image('output', output, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
    "        writer_val.add_scalar('loss', np.mean(loss_arr), epoch)\n",
    "        \n",
    "        save(ckpt_dir=ckpt_dir, net=net, optim=optim, epoch=epoch)\n",
    "    writer_train.close()\n",
    "    writer_val.close()\n",
    "     \n",
    "else:\n",
    "    # test nework\n",
    "    net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        loss_arr = []\n",
    "        \n",
    "        for batch, data in enumerate(loader_test, 1):\n",
    "            \n",
    "            # forward pass\n",
    "            label = data['label'].to(device)\n",
    "            input = data['input'].to(device)\n",
    "            \n",
    "            output = net(input)\n",
    "            \n",
    "            #calculate loss function\n",
    "            loss = fn_loss(output, label)\n",
    "            \n",
    "            loss_arr += [loss.item()]\n",
    "            \n",
    "            print(\"Test: Batch %04d / %04d | Loss %.4f\" %(batch, num_batch_test, np.mean(loss_arr)))\n",
    "            \n",
    "            # save Tensorboard \n",
    "            label = fn_tonumpy(label)\n",
    "            input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
    "            output = fn_tonumpy(fn_class(output))\n",
    "            \n",
    "            for j in range(label.shape[0]):\n",
    "                id = num_batch_test * (batch - 1) + j\n",
    "                \n",
    "                plt.imsave(os.path.join(result_dir, 'png', 'label_%04d.png' %id), label[j].squeeze(), cmap='gray')\n",
    "                plt.imsave(os.path.join(result_dir, 'png', 'input_%04d.png' %id), input[j].squeeze(), cmap='gray')\n",
    "                plt.imsave(os.path.join(result_dir, 'png', 'output_%04d.png' %id), output[j].squeeze(), cmap='gray')\n",
    "                \n",
    "                np.save(os.path.join(result_dir, 'numpy', 'label_%04d.npy' %id), label[j].squeeze())\n",
    "                np.save(os.path.join(result_dir, 'numpy', 'input_%04d.npy' %id), input[j].squeeze())\n",
    "                np.save(os.path.join(result_dir, 'numpy', 'output_%04d.npy' %id), output[j].squeeze())\n",
    "                \n",
    "    print(\"Average: Batch %04d / %04d | Loss %.4f\" %(batch, num_batch_test, np.mean(loss_arr)))   "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02eddd94fd9f0d372956d3dca77a19e74c96a8fd13e5de75381a35868b9e5fc1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
