{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on the CPU\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_epoch = 100\n",
    "data_dir = \"C:/Users/hjane/Desktop/Git/Unet/data\"\n",
    "ckpt_dir = \"C:/Users/hjane/Desktop/Git/Unet/checkpoint\"\n",
    "log_dir = \"C:/Users/hjane/Desktop/Git/Unet/log\"\n",
    "result_dir = \"C:/Users/hjane/Desktop/Git/Unet/results\"\n",
    "\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(os.path.join(result_dir, 'png'))\n",
    "    os.makedirs(os.path.join(result_dir, 'numpy'))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"running on the CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1200/1*qNdglJ1ORP3Gq77MmBLhHQ.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        def CBR2d(in_channnels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channnels, out_channels=out_channels, \n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)]\n",
    "            \n",
    "            layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            layers += [nn.ReLU()]\n",
    "            \n",
    "            cbr = nn.Sequential(*layers)\n",
    "            \n",
    "            return cbr\n",
    "\n",
    "        # Contracting path\n",
    "        self.enc1_1 = CBR2d(in_channnels=1, out_channels=64)\n",
    "        self.enc1_2 = CBR2d(in_channnels=64, out_channels=64)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.enc2_1 = CBR2d(in_channnels=64, out_channels=128)\n",
    "        self.enc2_2 = CBR2d(in_channnels=128, out_channels=128)\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.enc3_1 = CBR2d(in_channnels=128, out_channels=256)\n",
    "        self.enc3_2 = CBR2d(in_channnels=256, out_channels=256)\n",
    "        \n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.enc4_1 = CBR2d(in_channnels=256, out_channels=512)\n",
    "        self.enc4_2 = CBR2d(in_channnels=512, out_channels=512)\n",
    "        \n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.enc5_1 = CBR2d(in_channnels=512, out_channels=1024)\n",
    "        \n",
    "        # Expansive path\n",
    "        self.dec5_1 = CBR2d(in_channnels=1024, out_channels=512)\n",
    "        \n",
    "        self.uppool4 = nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=2, stride=2, padding=0, bias=True)\n",
    "        \n",
    "        self.dec4_2 = CBR2d(in_channnels=2 * 512, out_channels=512)\n",
    "        self.dec4_1 = CBR2d(in_channnels=512, out_channels=256)\n",
    "        \n",
    "        self.uppool3 = nn.ConvTranspose2d(in_channels=256, out_channels=256, kernel_size=2, stride=2, padding=0, bias=True)\n",
    "        \n",
    "        self.dec3_2 = CBR2d(in_channnels=2*256, out_channels=256)\n",
    "        self.dec3_1 = CBR2d(in_channnels=256, out_channels=128)\n",
    "        \n",
    "        self.uppool2 = nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=2, stride=2, padding=0, bias=True)\n",
    "        \n",
    "        self.dec2_2 = CBR2d(in_channnels=2*128, out_channels=128)\n",
    "        self.dec2_1 = CBR2d(in_channnels=128, out_channels=64)\n",
    "        \n",
    "        self.uppool1 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=2, stride=2, padding=0, bias=True)\n",
    "        \n",
    "        self.dec1_2 = CBR2d(in_channnels=2*64, out_channels=64)\n",
    "        self.dec1_1 = CBR2d(in_channnels=64, out_channels=64)\n",
    "        \n",
    "        self.fc = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "        \n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "        \n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "        \n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "        \n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        \n",
    "        dec5_1 =self.dec5_1(enc5_1)\n",
    "        \n",
    "        uppool4 = self.uppool4(dec5_1)\n",
    "        \n",
    "        dec4_2 = self.dec4_2(torch.cat((uppool4, enc4_2), dim=1))\n",
    "        dec4_1 = self.dec4_1(dec4_2)\n",
    "        \n",
    "        uppool3 = self.uppool3(dec4_1)\n",
    "        \n",
    "        dec3_2 = self.dec3_2(torch.cat((uppool3, enc3_2), dim=1))\n",
    "        dec3_1 = self.dec3_1(dec3_2)\n",
    "        \n",
    "        uppool2 = self.uppool2(dec3_1)\n",
    "        \n",
    "        dec2_2 = self.dec2_2(torch.cat((uppool2, enc2_2), dim=1))\n",
    "        dec2_1 =self.dec2_1(dec2_2)\n",
    "        \n",
    "        uppool1 = self.uppool1(dec2_1)\n",
    "        \n",
    "        dec1_2 = self.dec1_2(torch.cat((uppool1, enc1_2), dim=1))\n",
    "        dec1_1 = self.dec1_1(dec1_2)\n",
    "        \n",
    "        x = self.fc(dec1_1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernel_size(Convolution의 view, 2차원에서는 대부분 3x3 pixel), stride(이미지를 스캔할 때 kernel의 step size), bias는 CBR layer를 구축하는데 있어 값이 고정됨 >> 이 값들이 뭐하는 얘들이지?\n",
    "각각의 함수가 의미하는 transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        lst_data = os.listdir(self.data_dir)\n",
    "\n",
    "        lst_label = [f for f in lst_data if f.startswith('label')]\n",
    "        lst_input = [f for f in lst_data if f.startswith('input')]\n",
    "\n",
    "        lst_label.sort()\n",
    "        lst_input.sort()\n",
    "\n",
    "        self.lst_label = lst_label\n",
    "        self.lst_input = lst_input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lst_label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = np.load(os.path.join(self.data_dir, self.lst_label[index]))\n",
    "        input = np.load(os.path.join(self.data_dir, self.lst_input[index]))\n",
    "\n",
    "        label = label/255.0\n",
    "        input = input/255.0\n",
    "\n",
    "        if label.ndim == 2:\n",
    "            label = label[:, :, np.newaxis]\n",
    "        if input.ndim == 2:\n",
    "            input = input[:, :, np.newaxis]\n",
    "\n",
    "        data = {'input': input, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "class ToTensor(object):\n",
    "    def __call__(self, data):\n",
    "        label, input = data['label'], data['input']\n",
    "\n",
    "        label = label.transpose((2, 0, 1)).astype(np.float32)\n",
    "        input = input.transpose((2, 0, 1)).astype(np.float32)\n",
    "\n",
    "        data = {'label': torch.from_numpy(label), 'input': torch.from_numpy(input)}\n",
    "\n",
    "        return data\n",
    "        \n",
    "class Normalization(object):\n",
    "    def __init__(self, mean=0.5, std=0.5):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        label, input = data['label'], data['input']\n",
    "        \n",
    "        input = (input - self.mean) / self.std\n",
    "        \n",
    "        data = {'label' : label, 'input' : input}\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    Normalization(mean=0.5, std=0.5),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "dataset_test = Dataset(data_dir=os.path.join(data_dir, 'test'), transform=transform)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build network\n",
    "net = UNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "fn_loss = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimizer\n",
    "optim = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables\n",
    "num_data_test = len(dataset_test)\n",
    "\n",
    "num_batch_test = np.ceil(num_data_test / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub functions\n",
    "\n",
    "fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\n",
    "fn_denorm = lambda x, mean, std: (x * std) + mean\n",
    "fn_class = lambda x: 1.0 * (x > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set SummaryWriter for Tensorboard\n",
    "writer_test = SummaryWriter(log_dir=os.path.join(log_dir, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save network\n",
    "def save(ckpt_dir, net, optim, epoch):\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "        \n",
    "    torch.save({'net' : net.state_dict(), 'optim': optim.state_dict()}, './%s/model_epoch%d.pth' % (ckpt_dir, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load network\n",
    "def load(ckpt_dir, net, optim):\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        epoch = 0\n",
    "        return net, optim, epoch\n",
    "    ckpt_lst = os.listdir(ckpt_dir)\n",
    "    ckpt_lst.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "    \n",
    "    dict_model = torch.load('./%s/%s' %(ckpt_dir, ckpt_lst[-1]))\n",
    "    \n",
    "    net.load_state_dict(dict_model['net'])\n",
    "    optim.load_state_dict(dict_model['optim'])\n",
    "    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n",
    "\n",
    "    return net, optim, epoch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_epoch =0\n",
    "net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not all arguments converted during string formatting",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hjane\\Desktop\\Git\\Unet\\eval.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hjane/Desktop/Git/Unet/eval.ipynb#ch0000019?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m fn_loss(output, label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hjane/Desktop/Git/Unet/eval.ipynb#ch0000019?line=19'>20</a>\u001b[0m loss_arr \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [loss\u001b[39m.\u001b[39mitem()]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hjane/Desktop/Git/Unet/eval.ipynb#ch0000019?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39;49m\u001b[39mTest: Batch $04d / \u001b[39;49m\u001b[39m%04d\u001b[39;49;00m\u001b[39m | Loss \u001b[39;49m\u001b[39m%.4f\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m(batch, num_batch_test, np\u001b[39m.\u001b[39;49mmean(loss_arr)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hjane/Desktop/Git/Unet/eval.ipynb#ch0000019?line=23'>24</a>\u001b[0m \u001b[39m# save Tensorboard \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hjane/Desktop/Git/Unet/eval.ipynb#ch0000019?line=24'>25</a>\u001b[0m label \u001b[39m=\u001b[39m fn_tonumpy(label)\n",
      "\u001b[1;31mTypeError\u001b[0m: not all arguments converted during string formatting"
     ]
    }
   ],
   "source": [
    "# train nework\n",
    "\n",
    "st_epoch =0\n",
    "net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    loss_arr = []\n",
    "    \n",
    "    for batch, data in enumerate(loader_test, 1):\n",
    "        \n",
    "        # forward pass\n",
    "        label = data['label'].to(device)\n",
    "        input = data['input'].to(device)\n",
    "        \n",
    "        output = net(input)\n",
    "        \n",
    "        #calculate loss function\n",
    "        loss = fn_loss(output, label)\n",
    "        \n",
    "        loss_arr += [loss.item()]\n",
    "        \n",
    "        print(\"Test: Batch %04d / %04d | Loss %.4f\" %(batch, num_batch_test, np.mean(loss_arr)))\n",
    "        \n",
    "        # save Tensorboard \n",
    "        label = fn_tonumpy(label)\n",
    "        input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
    "        output = fn_tonumpy(fn_class(output))\n",
    "        \n",
    "        for j in range(label.shape[0]):\n",
    "            id = num_batch_test * (batch - 1) + j\n",
    "            \n",
    "            plt.imsave(os.path.join(result_dir, 'png', 'label_%04d.png' %id), label[j].squeeze(), cmap='gray')\n",
    "            plt.imsave(os.path.join(result_dir, 'png', 'input_%04d.png' %id), input[j].squeeze(), cmap='gray')\n",
    "            plt.imsave(os.path.join(result_dir, 'png', 'output_%04d.png' %id), output[j].squeeze(), cmap='gray')\n",
    "            \n",
    "            np.save(os.path.join(result_dir, 'numpy', 'label_%04d.npy' %id), label[j].squeeze())\n",
    "            np.save(os.path.join(result_dir, 'numpy', 'input_%04d.npy' %id), input[j].squeeze())\n",
    "            np.save(os.path.join(result_dir, 'numpy', 'output_%04d.npy' %id), output[j].squeeze())\n",
    "            \n",
    "print(\"Average: Batch %04d / %04d | Loss %.4f\" %(batch, num_batch_test, np.mean(loss_arr)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02eddd94fd9f0d372956d3dca77a19e74c96a8fd13e5de75381a35868b9e5fc1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
